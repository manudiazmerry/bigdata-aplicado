{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d02526",
   "metadata": {},
   "source": [
    "### Dataframes, Spark SQL e Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temos varias formas de crear un DataFrame:\n",
    "# - a partir dunha lista de datos\n",
    "# - lectura de ficheiros (diferentes formatos)\n",
    "# -- Local Filesystem\n",
    "# -- HDFS\n",
    "# -- nube: S3 Azure, HBase, Mysql ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6d89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9e20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df. show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unha das novidades de Spark SQL é que permite facer consultas sobre as táboas/Dataframees como se\n",
    "# fosen táboas dunha base de datos relacional, utilizando ANSI SQL\n",
    "\n",
    "# O primeiro que hai que facer é crear unha táboa temporal sobre o Dataframe con 'createOrReplaceTempView()'\n",
    "# A continuación poderanse executar sentencias SQL a través da función 'sql()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79135f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('employees')\n",
    "spark.sql('SELECT * FROM employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309b6c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos gardar o resultado como un novo dataframe\n",
    "resultado = spark.sql('SELECT * FROM employees')\n",
    "resultado.printSchema()\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e259247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+\n",
      "|firstname|middlename|salary|\n",
      "+---------+----------+------+\n",
      "|  Michael|      Rose|  4000|\n",
      "|   Robert|          |  4000|\n",
      "|    Maria|      Anne|  4000|\n",
      "+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos filtrar con condicións\n",
    "spark.sql(\"SELECT firstname, middlename, salary FROM employees WHERE salary > 3500\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28d38594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     M|       3|\n",
      "|     F|       2|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos realizar funcións de agrupación\n",
    "spark.sql(\"SELECT gender, count(*) FROM employees GROUP BY gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En resumo, spark.sql permítenos traballar cos Dataframes como se de táboas relacionais se tratase\n",
    "# e utilizar SQL para realizar as consultas que precisemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3116bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ademais dos ficheiros de texto CSV, tsv, xml, json... Spark pode traballar con outros formatos\n",
    "# Algúns formatos moi empregados son Avro ou Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c7b1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Parquet é un formato de almacenamento con almacenamento columnar. Esta característica fai\n",
    "# que sexa moi rápido no procesado de consultas de agregación\n",
    "# PySpark soporta Parquet de xeito nativo, sen necesidade de icorporar novas librarías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eea43a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Escritura a formato parquet\n",
    "df.write.parquet('file:///tmp/employees.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a492fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lectura de format parquet\n",
    "df_parquet = spark.read.parquet('file:///tmp/employees.parquet')\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unhas das particularidades de Parquet é que permite crear particións, de xeito que os datos\n",
    "# se poden almacenar en ficheiros separados en función das nosas necesidades, o que pode mellorar\n",
    "# o rendemento en certas consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos crear particións para os datos, separando os datos por 'gender' e 'salary' en ficheiros diferentes\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"file:///tmp/output/employees2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d58b389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos cargar o ficheiro completo\n",
    "df_parquet_partido = spark.read.parquet(\"file:///tmp/output/employees2.parquet\")\n",
    "df_parquet_partido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51fb383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+\n",
      "|firstname|middlename|lastname|       dob|salary|\n",
      "+---------+----------+--------+----------+------+\n",
      "|   Robert|          |Williams|1978-09-05|  4000|\n",
      "|  Michael|      Rose|        |2000-05-19|  4000|\n",
      "|    James|          |   Smith|1991-04-01|  3000|\n",
      "+---------+----------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos cargar só unha partición (a partición de )gender=M)\n",
    "df_parquet_partido = spark.read.parquet(\"file:///tmp/output/employees2.parquet/gender=M\")\n",
    "df_parquet_partido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3f9fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+\n",
      "|firstname|middlename|lastname|       dob|\n",
      "+---------+----------+--------+----------+\n",
      "|   Robert|          |Williams|1978-09-05|\n",
      "|  Michael|      Rose|        |2000-05-19|\n",
      "+---------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos cargar só unha partición máis específica\n",
    "# Fíxate como xa non aparecen os campos\n",
    "df_parquet_partido = spark.read.parquet(\"file:///tmp/output/employees2.parquet/gender=M/salary=4000\")\n",
    "df_parquet_partido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6614d5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec70eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
